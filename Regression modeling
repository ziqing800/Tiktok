# Import packages for data preprocessing
from sklearn.preprocessing import OneHotEncoder
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.utils import resample

# Import packages for data modeling
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Check class balance 
data["verified_status"].value_counts(normalize=True)

# Approximately 93.6% of the dataset represents videos posted by unverified accounts and 6.4% represents videos posted by verified accounts. 
# So the outcome variable is not very balanced.

# Use resampling to create class balance in the outcome variable

# Identify data points from majority and minority classes
data_majority = data[data["verified_status"] == "not verified"]
data_minority = data[data["verified_status"] == "verified"]

# Upsample the minority class (which is "verified")
data_minority_upsampled = resample(data_minority,
                                 replace=True,                 # to sample with replacement
                                 n_samples=len(data_majority), # to match majority class
                                 random_state=0)               # to create reproducible results

# Combine majority class with upsampled minority class
data_upsampled = pd.concat([data_majority, data_minority_upsampled]).reset_index(drop=True)

# Display new class counts
data_upsampled["verified_status"].value_counts()

# Get the average `video_transcription_text` length for verified and the average `video_transcription_text` length for unverified
data_upsampled[["verified_status", "video_transcription_text"]].groupby(by="verified_status")[["video_transcription_text"]].agg(func=lambda array: np.mean([len(text) for text in array]))
                                                                                                                                                            
# Extract the length of each `video_transcription_text` and add this as a column to the dataframe
data_upsampled["length_text"]=data_upsampled["video_transcription_text"].apply(func=lambda text: len(text))
                                                                                                                                                            
# Visualize the distribution of `video_transcription_text` length for videos posted by verified accounts and videos posted by unverified accounts
# Create two histograms in one plot
sns.histplot(data=data_upsampled, x="length_text", hue="verified_status", multiple='stack')
plt.title("Distribution of text length for videos posted by verified accounts and videos posted by unverified accounts") 
                                                                                                                                                            
# Examine correlations 
# Code a correlation matrix to help determine most correlated variables
data_upsampled.corr(numeric_only=True)

# Create a heatmap to visualize how correlated variables are
plt.figure(figsize=(8, 6))
sns.heatmap(
    data_upsampled[["video_duration_sec", "claim_status", "author_ban_status", "video_view_count", 
                    "video_like_count", "video_share_count", "video_download_count", "video_comment_count", "text_length"]]
    .corr(numeric_only=True), 
    annot=True, 
    cmap="crest")
plt.title("Heatmap of the dataset")
plt.show()

# The above heatmap shows that the following pair of variables are strongly correlated: video_comment_count and video_download_count (0.86 correlation coefficient).
# One of the model assumptions for logistic regression is no severe multicollinearity among the features. 
# To build a logistic regression model that meets this assumption, exclude vide_download_count. 
# Among the variables that quantify video metrics, keep video_view_count, video_share_count, vide0_like_count, and video_comment_count as features.

# Construct the model 

# Select outcome variable
y=data_upsampled["verified_status"]
# Select features
x=data_upsampled[["video_duration_sec", "claim_status", "author_ban_status","video_view_count", "video_share_count", "video_like_count", "video_comment_count"]]

# Split the data into training and testing sets
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=42)

# Get shape of each training and testing set to confirm the dimensions are alligned
x_train.shape,x_test.shape,y_train.shape,y_test.shape

# Get shape of each training and testing set
x_train.shape,x_test.shape,y_train.shape,y_test.shape

# Get unique values in `claim_status`
x_train["claim_status"].unique()

# Get unique values in `author_ban_status`
x_train["author_ban_status"].unique()

# Select the training features that needs to be encoded
x_to_encode=x_train[["author_ban_status", "claim_status"]]

# Set up an encoder for one-hot encoding the categorical features
# Sparse_output=False returns an array
x_encoder=OneHotEncoder(drop='first',sparse_output=False)

# Fit and transform the training features using the encoder
x_train_encoded=x_encoder.fit_transform(x_to_encode)

# Get feature names from encoder
x_encoder.get_feature_names_out()

# Place encoded training features (which is currently an array) into a dataframe
x_train_encoded=pd.DataFrame(data=x_train_encoded,columns=x_encoder.get_feature_names_out())

# Display first few rows of `X_train` with `claim_status` and `author_ban_status` columns dropped (since these features are being transformed to numeric)
x_train.drop(columns=["claim_status","author_ban_status"]).head()

# Concatenate `X_train` and `X_train_encoded_df` to form the final dataframe for training data (`X_train_final`)
# Using `.reset_index(drop=True)` to reset the index in X_train after dropping `claim_status` and `author_ban_status`,
# so that the indices align with those in `X_train_encoded_df` and `count_df`
x_train_final=pd.concat([x_train.drop(columns=["claim_status", "author_ban_status"]).reset_index(drop=True),x_train_encoded],axis=1)

# Get unique values of outcome variable
y_train.unique()

# Set up an encoder for one-hot encoding the categorical outcome variable
y_encoder=OneHotEncoder(drop="first",sparse_output=False)

# Encode the training outcome variable
y_train_final = y_encoder.fit_transform(y_train.values.reshape(-1, 1)).ravel()

# Construct a logistic regression model and fit it to the training set
clf=LogisticRegression().fit(x_train_final,y_train_final)

# Select the testing features that needs to be encoded
x_test_to_encode=x_test[["author_ban_status", "claim_status"]]

# Transform the testing features using the encoder
x_test_encoded=x_encoder.transform(x_test_to_encode)

# Place encoded testing features (which is currently an array) into a dataframe
x_test_encoded_df=pd.DataFrame(data=x_test_encoded,columns=x_encoder.get_feature_names_out())

# Display first few rows of `X_test` with `claim_status` and `author_ban_status` columns dropped (since these features are being transformed to numeric)
x_test.drop(["claim_status","author_ban_status"],axis=1).head()

# Concatenate `X_test` and `X_test_encoded_df` to form the final dataframe for training data (`X_test_final`)
x_test_final=pd.concat([x_test.drop(["claim_status","author_ban_status"],axis=1).reset_index(drop=True),x_test_encoded_df],axis=1)

# Use the logistic regression model to get predictions on the encoded testing set
y_pred=clf.predict(x_test_final)

# Encode the testing outcome variable
y_test_final=y_encoder.transform(y_test.values.reshape(-1, 1)).ravel()

# Visualize the model results 

# Compute values for confusion matrix
cm=confusion_matrix(y_test_final,y_pred,labels=clf.classes_)

# Create display of confusion matrix
disp=ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=clf.classes_)

# Plot confusion matrix
disp.plot()

# Display plot
plt.show()

#(4555+2494)/(2494+2859+823+4555)
#0.6568819308545336

# Create a classification report
target_labels = ["verified", "not verified"]
print(classification_report(y_test_final, y_pred, target_names=target_labels))

# Interpret model coefficients 
# Get the feature names from the model and the model coefficients (which represent log-odds ratios)
# Place into a DataFrame for readability

pd.DataFrame(data={"Feature Name":clf.feature_names_in_, "Model Coefficient":clf.coef_[0]})
